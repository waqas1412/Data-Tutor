export interface LessonData {
  id: number;
  section: string;
  image?: string;
}

export const lessonData: LessonData[] = [
  {
      "id": 1,
      "section": "Alright, data enthusiast\\! Let's embark on this exciting journey through the world of data engineering. I've tailored this course into 30 days of learning modules, each packed with bite-sized chunks of data goodness. Buckle up, because we're about to turn you into a data modeling wizard\\! \ud83e\uddd9\u200d\u2642\ufe0f\ud83d\udcbe"
  },
  {
      "id": 2,
      "section": "# Day 0-7: Computer Science Fundamentals\n\nNote: If you're completely new to the world of Computer Science (which is perfectly fine, we've all been there), I highly recommend Harvard's CS50 Introduction to Computer Science on YouTube. David Malan delivers a charismatic, engaging, and passionate exploration of all the fundamentals of CS, accompanied by practical examples. I would give it a solid 10/10.  \nHere is the link: [https://www.youtube.com/watch?v=IDDmrzzB14M\\&list=PLhQjrBD2T380F\\_inVRXMIHCqLaNUd7bN4\\&ab\\_channel=CS50](https://www.youtube.com/watch?v=IDDmrzzB14M&list=PLhQjrBD2T380F_inVRXMIHCqLaNUd7bN4&ab_channel=CS50)\n\nThe main concepts you need to know are: GIT, APIs, CLI, Structured & Unstructured data, SSH, Linux, Shell scripting, Cron jobs",
      "image": "images/lessons/Fundamentals.jpg"
  },
  {
      "id": 3,
      "section": "## Module 1: GIT \n\nWhat it is: GIT is like that magical \"Undo\" button you wish you had in life, but for code. It\u2019s a version control system that tracks your changes and helps you collaborate with others without stepping on their toes (or code).\n\nWhat you need to know:\n\n* Basic commands like `clone`, `commit`, `push`, and `pull`.  \n* How to create and merge branches. (No, not tree branches, but kind of similar in concept.)  \n* Resolve conflicts when your coworker\u2019s code doesn\u2019t play nice with yours.\n\n**Why it matters:** In data engineering, your code and pipelines are always evolving. GIT ensures you don\u2019t accidentally delete the entire database schema at 2 AM (we\u2019ve all been there).\n\nWhat You Need to Know in GIT\n\n#### 1\\. Basic Workflow Commands\n\nThese are the bread and butter of GIT.\n\n* **`git init`**: Start tracking changes in a folder. It creates a `.git` folder.  \n  *Think of it as planting the GIT flag to claim your territory.*  \n* **`git add <file>`**: Stage your changes, preparing them for a snapshot.  \n* **`git commit -m \"message\"`**: Save a snapshot of your work with a message.  \n* **`git status`**: See what\u2019s changed or what\u2019s staged for commit.  \n  *Your personal assistant for what\u2019s going on in your repo.*  \n* **`git log`**: View the history of commits.\n\n#### **2\\. Collaborating with Repositories**\n\n* **`git clone <repo-url>`**: Copy a remote repository to your local machine.  \n  *Like downloading your friend\u2019s homework but legally.*  \n* **`git pull`**: Update your local repository with changes from the remote.  \n  *Fetching the latest gossip from the team repo.*  \n* **`git push`**: Upload your local commits to the remote repository.  \n  *Sharing your hard work with the world\u2014or at least your team.*\n\n#### **3\\. Branching and Merging**\n\nBranches let you work on new features or fixes without breaking the main code.\n\n* **`git branch <branch-name>`**: Create a new branch.  \n* **`git checkout <branch-name>`**: Switch to a different branch.  \n* **`git merge <branch-name>`**: Combine changes from one branch into another.  \n  *When your experimental recipe turns out great, you add it to the main menu.*  \n* **`git branch -d <branch-name>`**: Delete a branch.\n\n#### **4\\. Resolving Conflicts**\n\nWhen two people edit the same file in different ways, GIT asks you to resolve the conflict.\n\n* **Open the conflicted file**: GIT marks the conflicting sections.  \n* **Choose or combine changes**: Decide what stays and what goes.  \n* **Commit the resolved file**: Save the final version.\n\n---\n\n### **Advanced Commands (You\u2019ll Eventually Use)**\n\n* **`git stash`**: Temporarily save changes without committing. Useful if your boss interrupts you mid-flow.  \n* **`git rebase`**: Reorganize commit history to make it cleaner (like tidying your closet).  \n* **`git reset`**: Undo commits (carefully\\!).\n\n---\n\n### **Best Practices for GIT as a Data Engineer**\n\n1. **Commit Often:** Think of commits as diary entries\u2014you\u2019ll want to track your progress.  \n2. **Write Clear Commit Messages:** Use verbs: e.g., `Added`, `Fixed`, `Refactored`.  \n3. **Pull Before Pushing:** Always fetch the latest changes to avoid conflicts.  \n4. **Use Branches:** Never work directly on the `main` branch; it\u2019s sacred\n\nIf you want to practice, use this Free GIT game: https://profy.dev/project/github-minesweeper",
  "image": "images/lessons/git.png"
  },
  {
      "id": 4,
      "section": "## Module 2:  APIs \u2013 Your Data's Delivery Guys\n\nWhat it is: APIs are like waiters\u2014they fetch data from somewhere else and deliver it to you. \n\n**What you need to know:**\n\n* How to send HTTP requests (`GET`, `POST`, `PUT`, `DELETE`) using tools like Postman or Python.  \n* Understand JSON responses\u2014they\u2019re like the menu of what data you can get.  \n* Authentication (because some APIs are exclusive).\n\n**Why it matters:** APIs are often how you\u2019ll fetch external data for your pipelines. Knowing how to handle them will make you feel like the data whisperer.\n\nAs a data engineer, you\u2019ll encounter APIs when:\n\n* Pulling data from external systems (e.g., weather data, stock prices, or social media metrics).  \n* Sending data to other systems (e.g., uploading processed data to a dashboard).  \n* Automating tasks across different platforms.\n\n---\n\n### **What You Need to Know About APIs**\n\n#### **1\\. How APIs Work**\n\nThink of APIs as a conversation between you (the client) and a system (the server). You ask for something in a specific way, and if you\u2019re polite (follow the API rules), the server responds.\n\n##### **The Basics:**\n\n* **Request:** Your message to the server.  \n  Example: \u201cHey, can I get today\u2019s weather?\u201d  \n* **Response:** The server\u2019s reply.  \n  Example: \u201cSure, it\u2019s sunny with 25\u00b0C\\!\u201d\n\n##### **HTTP Requests:**\n\n* **GET:** Fetch data.  \n  *\u201cCan I have the latest stock prices?\u201d*  \n* **POST:** Send data.  \n  *\u201cHere\u2019s the new user I just created.\u201d*  \n* **PUT:** Update existing data.  \n  *\u201cChange my profile picture to this new one.\u201d*  \n* **DELETE:** Remove data.  \n  *\u201cPlease delete this old report.\u201d*\n\n---\n\n#### **2\\. Tools to Work with APIs**\n\n* **Postman**: A user-friendly tool to test API calls without writing code. Think of it as your API sandbox.  \n* **Curl (CLI):** A command-line tool for making API requests. Perfect for quick testing.  \n* **Python (or other languages):** Use libraries like `requests` in Python to integrate APIs into your workflows.\n\n---\n\n#### **3\\. Key Concepts for APIs**\n\n* **Endpoints:** The URL of the API.  \n  Example: `https://api.openweathermap.org/data/2.5/weather`  \n* **Parameters:** Extra information you send with a request to customize the response.  \n  Example: `?q=London&units=metric` (gets the weather for London in metric units).  \n* **Headers:** Metadata sent with a request, often for authentication.  \n  Example: `Authorization: Bearer <your_token>`\n\n**JSON:** The most common data format APIs use.  \nExample:  \njson  \n`{`\n\n  `\"temperature\": 25,`\n\n  `\"condition\": \"Sunny\"`\n\n`}`\n\n---\n\n#### **4\\. Authentication**\n\nMany APIs require you to prove who you are, like showing your ID at the door.\n\n* **API Keys:** A unique key that identifies you.  \n* **OAuth Tokens:** Temporary tokens for secure access.  \n* **Basic Auth:** Username and password (not very common these days).\n\n---\n\n#### **5\\. Error Handling**\n\nAPIs aren\u2019t perfect; they\u2019ll sometimes throw tantrums. Learn to handle errors gracefully:\n\n* **Status Codes:**  \n  * `200 OK`: Success\\! \ud83c\udf89  \n  * `404 Not Found`: You\u2019re asking for something that doesn\u2019t exist.  \n  * `500 Internal Server Error`: The server had a bad day\u2014try again later.  \n* **Retry Logic:** For temporary errors, retry after a short delay.  \n* **Rate Limits:** Some APIs limit the number of requests you can make in a given time.\n\n---\n\n### **Essential Python Commands for APIs**\n\nHere\u2019s how you might interact with APIs using Python:\n\n\\`\\`\\`\n\n`import requests`\n\n`# Example GET request`\n\n`url = \"https://api.openweathermap.org/data/2.5/weather\"`\n\n`params = {\"q\": \"London\", \"units\": \"metric\", \"appid\": \"<your_api_key>\"}`\n\n`response = requests.get(url, params=params)`\n\n`if response.status_code == 200:`\n\n    `data = response.json()`\n\n    `print(f\"Temperature: {data['main']['temp']}\u00b0C\")`\n\n`else:`\n\n    `print(f\"Error: {response.status_code}\")`\n\n```` ``` ````\n\n---\n\n### **Common Use Cases for APIs in Data Engineering**\n\n1. **Ingesting Data:** Fetching data from APIs for ETL pipelines (e.g., stock prices, weather data).  \n2. **Sending Processed Data:** Posting results to dashboards or notification systems.  \n3. **Triggering Workflows:** Calling APIs to initiate tasks or notify other systems.\n\n---\n\n### **Best Practices When Using APIs**\n\n1. **Read the Documentation:** API docs are your holy grail. They explain what\u2019s available and how to use it.  \n2. **Test Before Coding:** Use Postman to ensure your requests work before integrating them into your code.  \n3. **Handle Errors Gracefully:** Log errors and retry where appropriate.  \n4. **Cache Responses:** Save API responses locally for repeated requests to reduce API calls (and costs).\n\nHere is the great article if you want to read more: https://www.postman.com/what-is-an-api/"
  },
  {
      "id": 5,
      "section": "## Module 3: CLI (Command Line Interface) \u2013 The Black Box of Power\n\n**What it is:** The CLI is that intimidating black screen hackers use in movies (spoiler: it\u2019s not as cool as Hollywood makes it look).\n\n**What you need to know:**\n\n* Navigate files and directories (`ls`, `cd`, `mkdir`).  \n* Copy, move, and delete files (`cp`, `mv`, `rm`).  \n* Search for text in files with `grep` (a lifesaver for debugging).\n\n**Why it matters:** Data engineering often requires automating tasks. The CLI is faster and more efficient than clicking around like a lost puppy.\n\nAs a data engineer, you\u2019ll use the CLI to:\n\n* Manage files and directories.  \n* Interact with servers and databases.  \n* Run scripts and automate tasks.  \n* Debug pipelines and processes.\n\n---\n\n### **What You Need to Know About the CLI**\n\n#### **1\\. Basic Navigation Commands**\n\nThink of these as your GPS for the computer.\n\n* **`pwd`**: Print working directory.  \n  *\u201cWhere am I right now?\u201d*  \n* **`ls`**: List files and directories.  \n  *\u201cWhat\u2019s around me?\u201d*  \n  Example: `ls -l` (detailed list) or `ls -a` (show hidden files).  \n* **`cd <directory>`**: Change directory.  \n  *\u201cTake me to another folder.\u201d*  \n  Example: `cd ~/Documents` (go to the Documents folder).  \n* **`mkdir <directory>`**: Create a new directory.  \n  *\u201cI need a new folder for my project.\u201d*  \n* **`rm <file>`**: Remove a file (be careful\u2014there\u2019s no recycle bin here\\!).  \n  Example: `rm -rf <directory>` (delete a directory and its contents).\n\n---\n\n#### **2\\. File Manipulation Commands**\n\nGet hands-on with files.\n\n* **`cp <source> <destination>`**: Copy a file or directory.  \n  Example: `cp file1.txt backup_file1.txt`  \n* **`mv <source> <destination>`**: Move (or rename) a file.  \n  Example: `mv old_name.txt new_name.txt`  \n* **`cat <file>`**: Display the contents of a file.  \n  *Perfect for quickly checking logs or scripts.*  \n* **`nano <file>` or `vim <file>`**: Open a file for editing.  \n  *Bonus points if you can exit Vim without Googling.*\n\n---\n\n#### **3\\. Searching and Filtering**\n\nFind needles in your data haystack.\n\n* **`grep \"<pattern>\" <file>`**: Search for text within a file.  \n  Example: `grep \"error\" log.txt` (find all lines containing \u201cerror\u201d).  \n* **`find <directory> -name <filename>`**: Search for files.  \n  Example: `find /home -name \"report.csv\"`  \n* **`sort <file>`**: Sort a file\u2019s content.  \n* **`uniq`**: Remove duplicate lines from a sorted file.\n\n---\n\n#### **4\\. Permissions and Ownership**\n\nManage who can do what with your files.\n\n* **`chmod`**: Change file permissions.  \n  Example: `chmod 755 script.sh` (gives read, write, and execute permissions).  \n* **`chown`**: Change file ownership.  \n  Example: `chown user:group file.txt`\n\n---\n\n#### **5\\. Monitoring and Debugging**\n\nKeep an eye on what\u2019s happening in your system.\n\n* **`top`**: Show running processes and their resource usage.  \n  *Great for spotting resource hogs.*  \n* **`ps aux`**: List all running processes.  \n  Example: `ps aux | grep python` (find Python processes).  \n* **`df -h`**: Check disk space usage.  \n* **`free -h`**: Check memory usage.  \n* **`tail -f <file>`**: Continuously monitor the end of a file (e.g., logs).\n\n---\n\n#### **6\\. Networking**\n\nTalk to other computers like a boss.\n\n* **`ping <host>`**: Check if a server is reachable.  \n  Example: `ping google.com`  \n* **`curl <url>`**: Make HTTP requests (great for quick API checks).  \n* **`scp <source> <user@host:destination>`**: Securely copy files to/from a remote machine.  \n  Example: `scp file.txt user@remote_server:/path/to/destination`\n\n---\n\n#### **7\\. Automation with CLI**\n\nCombine commands to automate tasks.\n\n* **Pipes (`|`)**: Pass output from one command to another.  \n  Example: `ls | grep \".txt\"` (find all `.txt` files).  \n* **Redirection (`>`, `>>`)**: Save command output to a file.  \n  Example: `ls > file_list.txt`\n\n**Shell scripts:** Write a series of CLI commands in a `.sh` file and execute them.  \nExample:  \nbash  \nCopy code  \n`# backup.sh`\n\n`tar -czf backup.tar.gz /path/to/directory`\n\n* \n\n---\n\n### **Best Practices for CLI as a Data Engineer**\n\n1. **Learn Keyboard Shortcuts:** Use Tab for auto-completion and the up arrow to recall previous commands.  \n2. **Use Aliases:** Shorten frequently used commands.  \n   Example: Add `alias ll='ls -l'` to your `.bashrc` or `.zshrc`.  \n3. **Don\u2019t Run Commands as Root (Unless Necessary):** Save yourself from catastrophic mistakes.  \n4. **Create Logs for Automation:** Redirect outputs to log files when running automated tasks.\n\n---\n\n### **Why CLI Matters for Data Engineers**\n\n* You\u2019ll work with servers where GUIs aren\u2019t available.  \n* Scripts and pipelines often rely on CLI commands.  \n* Mastering the CLI makes you faster and more efficient.\n\nHere is a great article about it: [https://tutorial.djangogirls.org/en/intro\\_to\\_command\\_line/](https://tutorial.djangogirls.org/en/intro_to_command_line/)\n\nIn case you want to practice a little here is a cool free course at Datacamp: [https://datacamp.pxf.io/3J7J7B](https://datacamp.pxf.io/3J7J7B)"
  },
  {
      "id": 6,
      "section": "## Module 4: Structured & Unstructured Data \n\n### \n\n### **What it is:** Structured data is your perfectly organized Excel sheet. Unstructured data is your camera roll with memes, selfies, and screenshots of recipes you\u2019ll never try.\n\n**What you need to know:**\n\n* How to work with structured data (think tables in databases).  \n* Handle unstructured data like logs, images, or social media feeds.  \n* Convert unstructured chaos into structured insights using tools like regex or natural language processing.\n\n**Why it matters:** You\u2019ll be dealing with all kinds of data. Knowing the difference helps you decide the tools and methods to process it.\n\n### **Structured Data \u2013 The Spreadsheet Superstar**\n\nStructured data is like a well-organized Excel sheet: everything has a place, and it\u2019s easy to find what you\u2019re looking for. It\u2019s stored in a **relational database** with a fixed schema (tables, rows, and columns).\n\n#### **Characteristics of Structured Data:**\n\n* **Organized**: Data fits neatly into tables.  \n* **Searchable**: Query it using SQL (Structured Query Language).  \n* **Schema-based**: Rigid format; each column has a specific data type.  \n* **Examples**:  \n  * Sales data: Customer IDs, order amounts, and timestamps.  \n  * Employee records: Names, salaries, and hire dates.  \n  * Sensor readings: Temperature, humidity, and timestamps.\n\n#### **What You Need to Know About Structured Data:**\n\n* **SQL Basics**:  \n  * *Query data*: `SELECT * FROM sales WHERE order_date = '2025-01-01';`  \n  * *Insert data*: `INSERT INTO employees (name, salary) VALUES ('Alice', 70000);`  \n  * *Update data*: `UPDATE employees SET salary = 80000 WHERE name = 'Alice';`  \n  * *Delete data*: `DELETE FROM employees WHERE name = 'Bob';`  \n* **Relational Databases**: Learn tools like MySQL, PostgreSQL, or Snowflake.  \n* **Data Modeling**: Understand how to design schemas and relationships between tables (e.g., primary and foreign keys).\n\n---\n\n### **Unstructured Data \u2013 The Artistic Rebel**\n\nUnstructured data doesn\u2019t follow any specific format. It\u2019s messy, varied, and often full of hidden gems. This data type needs more work to analyze, but it holds massive potential.\n\n#### **Characteristics of Unstructured Data:**\n\n* **Flexible**: No fixed schema or format.  \n* **Harder to Process**: Requires special tools like Hadoop, Spark, or Python libraries.  \n* **Examples**:  \n  * Social media posts, tweets, and Instagram captions.  \n  * Images, videos, and audio files.  \n  * Logs from servers or applications.  \n  * Emails or chat messages.\n\n#### **What You Need to Know About Unstructured Data:**\n\n* **Storage**: Use tools like S3 (AWS), Google Cloud Storage, or Hadoop Distributed File System (HDFS).  \n* **Parsing & Extracting Insights**:  \n  * Text: Use Python libraries like `re` (regex) or NLP tools (e.g., spaCy, NLTK).  \n  * Images: Use computer vision frameworks like OpenCV or TensorFlow.  \n  * Logs: Tools like Elasticsearch or Splunk help analyze logs efficiently.  \n* **Data Cleaning**: Transform the raw mess into something usable.\n\n---\n\n### **Semi-Structured Data \u2013 The Middle Ground**\n\nSemi-structured data is like a well-meaning minimalist: it has some structure but isn\u2019t rigid. JSON and XML files are common examples.\n\n#### **Characteristics of Semi-Structured Data:**\n\n* **Flexible Schema**: Not as strict as relational databases but more organized than unstructured data.  \n* **Human-Readable**: Formats like JSON are easy to understand.  \n* **Examples**:\n\nAPI responses:\n\n\\`\\`\\`\n\njson  \n`{`\n\n  `\"id\": 123,`\n\n  `\"name\": \"Alice\",`\n\n  `\"purchases\": [\"Book\", \"Laptop\"]`\n\n`}`\n\n```` ``` ````\n\n* Configuration files (YAML, XML).\n\n#### **What You Need to Know About Semi-Structured Data:**\n\n* Parse JSON and XML using Python (`json` and `xml.etree.ElementTree`).  \n* Store and query in NoSQL databases like MongoDB or DynamoDB.  \n* Integrate it with tools like Kafka or Spark for ETL workflows.\n\n---\n\n### **How Structured and Unstructured Data Fit into Data Engineering**\n\n* **Ingesting Data:**  \n  * Structured: Pull from databases using SQL queries.  \n  * Unstructured: Scrape websites, process logs, or parse API responses.  \n* **Storing Data:**  \n  * Structured: Use relational databases.  \n  * Unstructured: Store in object storage (e.g., AWS S3).  \n* **Processing Data:**  \n  * Structured: Use SQL queries or ETL pipelines.  \n  * Unstructured: Process using Spark, Hadoop, or Python scripts.  \n* **Analyzing Data:**  \n  * Structured: Plug into BI tools like Tableau or Power BI.  \n  * Unstructured: Use AI/ML models to extract insights.\n\n---\n\n### **Best Practices for Working with Structured & Unstructured Data**\n\n1. **Understand Your Data Sources**: Know what kind of data you\u2019re dealing with before designing pipelines.  \n2. **Use the Right Tool for the Job**: Relational databases for structured data; distributed systems for unstructured.  \n3. **Validate Data**: Ensure structured data matches its schema; clean up unstructured data.  \n4. **Optimize Storage**: Use compression for unstructured data (e.g., Parquet, Avro).  \n5. **Stay Flexible**: Semi-structured data often acts as a bridge between the two worlds.\n\nHere is a great article about it: [https://www.altexsoft.com/blog/structured-unstructured-data/](https://www.altexsoft.com/blog/structured-unstructured-data/)"
  },
  {
      "id": 7,
      "section": "## Module 5: SSH \u2013 The Secure Key to Remote Machines\n\n**What it is:** SSH is like a long-distance relationship with servers\u2014it lets you connect to them securely from anywhere.\n\n**What you need to know:**\n\n* How to generate and use SSH keys for authentication.  \n* Login to remote servers (`ssh user@hostname`).  \n* Transfer files securely using `scp` or `rsync`.\n\n**Why it matters:** Data lives on servers, and SSH is your golden ticket to access them without anyone eavesdropping.\n\n### **What You Need to Know About SSH**\n\n#### **1\\. Why SSH?**\n\n* **Secure Communication**: SSH encrypts the connection, so your commands and data transfers are safe from eavesdroppers.  \n* **Remote Access**: Control servers without leaving your comfy chair.  \n* **File Transfers**: Move files securely between local and remote systems.\n\n---\n\n#### **2\\. Basic SSH Commands**\n\nMaster these, and you\u2019ll be remoting like a pro:\n\n**Connect to a Remote Server**\n\n\\`\\`\\`\n\nbash  \n`ssh username@remote_server`\n\n```` ``` ````\n\n* Example: `ssh natalie@192.168.1.10`\n\n**Use a Specific Port**  \nSometimes SSH isn\u2019t on the default port (22).  \nbash  \nCopy code  \n`ssh -p 2222 username@remote_server`\n\n* \n\n**Run a Command on a Remote Server**  \nbash  \nCopy code  \n`ssh username@remote_server \"command\"`\n\n* Example: `ssh natalie@192.168.1.10 \"ls -la\"`\n\n**Exit SSH Session**  \nbash  \nCopy code  \n`exit`\n\n* \n\n---\n\n#### **3\\. Managing SSH Keys**\n\nPasswords are pass\u00e9\u2014SSH keys are more secure and convenient.\n\n**Generate an SSH Key Pair**  \nbash  \nCopy code  \n`ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"`\n\n*   \n  * `-t rsa`: Specifies the key type (RSA).  \n  * `-b 4096`: Sets key strength to 4096 bits (more secure).  \n  * `-C`: Adds a label (your email).\n\n**Add the Key to a Remote Server**  \nbash  \nCopy code  \n`ssh-copy-id username@remote_server`\n\n* This copies your public key to the server, enabling passwordless login.\n\n**Use a Specific Private Key**  \nbash  \nCopy code  \n`ssh -i /path/to/private_key username@remote_server`\n\n* \n\n---\n\n#### **4\\. File Transfers with SSH**\n\nBecause sometimes you need to send files to or from remote machines.\n\n**SCP (Secure Copy Protocol)**: Copy files between local and remote systems.  \nbash  \nCopy code  \n`scp file.txt username@remote_server:/path/to/destination`\n\n* Example: `scp project.zip natalie@192.168.1.10:/home/natalie`\n\n**RSYNC**: For efficient file transfers (great for large datasets).  \nbash  \nCopy code  \n`rsync -avz local_dir/ username@remote_server:/remote_dir/`\n\n* Example: `rsync -avz data/ natalie@192.168.1.10:/data_backup/`\n\n---\n\n#### **5\\. SSH Configurations**\n\nSave time with SSH config files, so you don\u2019t have to type the full command every time.\n\nEdit or create a config file:  \nbash  \nCopy code  \n`nano ~/.ssh/config`\n\n* \n\nAdd an entry:  \nplaintext  \nCopy code  \n`Host myserver`\n\n    `HostName 192.168.1.10`\n\n    `User natalie`\n\n    `Port 22`\n\n    `IdentityFile ~/.ssh/id_rsa`\n\n* \n\nConnect with shorthand:  \nbash  \nCopy code  \n`ssh myserver`\n\n* \n\n---\n\n#### **6\\. Tunneling with SSH**\n\nSometimes, you need to route traffic securely through SSH.\n\n**Local Port Forwarding**: Access a remote resource via a local port.  \nbash  \nCopy code  \n`ssh -L local_port:target_host:target_port username@remote_server`\n\n* Example: `ssh -L 8080:localhost:3306 natalie@192.168.1.10`\n\n**Reverse Port Forwarding**: Expose a local resource to the remote server.  \nbash  \nCopy code  \n`ssh -R remote_port:localhost:local_port username@remote_server`\n\n* \n\n---\n\n#### **7\\. Debugging SSH**\n\nSSH not working? Try these tips:\n\n**Verbose Output**:  \nbash  \nCopy code  \n`ssh -v username@remote_server`\n\n* This shows detailed connection logs.\n\n**Check SSH Service**: Ensure the server\u2019s SSH service is running:  \nbash  \nCopy code  \n`sudo systemctl status ssh`\n\n*   \n* **Firewall Rules**: Ensure the server\u2019s firewall allows SSH connections on the specified port.\n\n---\n\n### **Why SSH Matters for Data Engineers**\n\n* **Server Management**: Quickly troubleshoot or configure remote systems.  \n* **Secure Data Pipelines**: Use SSH tunnels for encrypted data transfer.  \n* **File Movement**: Easily transfer datasets or code between local and cloud environments.\n\n---\n\n### **Best Practices for SSH**\n\n1. **Use SSH Keys**: Ditch passwords\u2014they\u2019re insecure and tedious.  \n2. **Disable Root Login**: Prevent unauthorized access by blocking direct root logins.  \n3. **Keep Keys Secure**: Never share your private key, and use passphrases for extra security.  \n4. **Audit Regularly**: Monitor logs to spot unusual SSH activity.  \n5. **Know Your Exit**: Always exit an SSH session when done (`exit`).\n\nHere is a cute video about ssh: https://www.youtube.com/watch?v=v45p\\_kJV9i4"
  },
  {
      "id": 8,
      "section": "## Module 6: Shell Scripting \u2013 Automation's Best Friend\n\n**What it is:** Shell scripts are like recipes\u2014they tell the computer what to do, step by step.\n\n### **What Is Shell Scripting?**\n\nA **shell script** is just a text file with a series of commands that the shell (your command-line interpreter) runs sequentially. It\u2019s your go-to tool for:\n\n* Automating repetitive tasks (e.g., backups, file processing).  \n* Managing servers and databases.  \n* Integrating multiple tools into a seamless workflow.\n\n---\n\n### **Why Shell Scripting for Data Engineers?**\n\n* **ETL Automation**: Move data between systems, transform it, and load it\u2014all from a script.  \n* **Job Scheduling**: Combine with tools like Cron for periodic tasks.  \n* **Environment Setup**: Automate setting up databases, directories, or dependencies.  \n* **Log Analysis**: Quickly parse logs and extract meaningful information.\n\n---\n\n### **Shell Scripting Basics**\n\n1. **Create a Shell Script**  \n   * Use any text editor to write your script (e.g., `nano`, `vim`).  \n   * Save it with a `.sh` extension.\n\nbash  \nCopy code  \n`nano my_script.sh`\n\n2.   \n3. **Make It Executable**\n\nGive your script permission to run:  \nbash  \nCopy code  \n`chmod +x my_script.sh`\n\n*   \n4. **Run the Script**\n\nExecute your script in the terminal:  \nbash  \nCopy code  \n`./my_script.sh`\n\n* \n\n---\n\n### **Sample Shell Script**\n\nHere\u2019s a script to back up files:\n\nbash\n\nCopy code\n\n`#!/bin/bash`\n\n`# Print the start message`\n\n`echo \"Starting backup process...\"`\n\n`# Variables`\n\n`SOURCE_DIR=\"/home/natalie/data\"`\n\n`DEST_DIR=\"/home/natalie/backup\"`\n\n`DATE=$(date +%Y-%m-%d)`\n\n`# Create backup directory if it doesn't exist`\n\n`mkdir -p $DEST_DIR`\n\n`# Copy files`\n\n`cp -r $SOURCE_DIR $DEST_DIR/backup_$DATE`\n\n`# Print completion message`\n\n`echo \"Backup completed successfully!\"`\n\n---\n\n### **Common Shell Commands in Scripts**\n\n**Variables**: Store and reuse values.  \nbash  \nCopy code  \n`NAME=\"Natalie\"`\n\n`echo \"Hello, $NAME!\"`\n\n* \n\n**Conditionals**: Add logic to your scripts.  \nbash  \nCopy code  \n`if [ -f \"file.txt\" ]; then`\n\n    `echo \"File exists.\"`\n\n`else`\n\n    `echo \"File not found.\"`\n\n`fi`\n\n* \n\n**Loops**: Iterate over files, directories, or commands.  \nbash  \nCopy code  \n`for file in *.txt; do`\n\n    `echo \"Processing $file\"`\n\n`done`\n\n* \n\n**Functions**: Reuse blocks of code.  \nbash  \nCopy code  \n`greet() {`\n\n    `echo \"Hello, $1!\"`\n\n`}`\n\n`greet \"Natalie\"`\n\n* \n\n---\n\n### **Key Tools and Commands**\n\n* **File Manipulation**:  \n  * Copy: `cp source destination`  \n  * Move: `mv source destination`  \n  * Delete: `rm file`  \n* **Text Processing**:  \n  * Search: `grep \"pattern\" file`  \n  * Sort: `sort file`  \n  * Replace: `sed 's/old/new/g' file`  \n* **Data Crunching**:  \n  * Count lines/words: `wc -l file`  \n  * Split files: `split -l 100 file`  \n  * Combine files: `cat file1 file2 > combined.txt`  \n* **System Monitoring**:  \n  * Disk usage: `du -sh directory`  \n  * Memory usage: `free -h`  \n  * Running processes: `top`\n\n---\n\n### **Debugging Shell Scripts**\n\nEven shell scripts can be divas. Here\u2019s how to keep them in check:\n\n**Run in Debug Mode**:  \nbash  \nCopy code  \n`bash -x my_script.sh`\n\n* This shows each command as it runs.\n\n**Check Syntax**:  \nbash  \nCopy code  \n`bash -n my_script.sh`\n\n*   \n* **Echo Statements**: Add `echo` to print variables and track progress.\n\n---\n\n### **Why Shell Scripting Is a Data Engineer's BFF**\n\n1. **Saves Time**: Automate mundane tasks and focus on the fun stuff.  \n2. **Scales Effortlessly**: Scripts can handle anything from a few files to massive datasets.  \n3. **Integrates Seamlessly**: Connects with tools like SSH, APIs, and databases.  \n4. **Reliable**: Scripts will do exactly what you tell them to (sometimes too literally\\!).\n\n---\n\n### **Best Practices for Shell Scripting**\n\n1. **Start with Comments**: Explain what each section of the script does.  \n   bash  \n   Copy code  \n   `# This script backs up data to a specific directory.`  \n2. **Use Variables**: Avoid hardcoding paths or values.\n\n**Error Handling**: Check for errors and handle them gracefully.  \nbash  \nCopy code  \n`if [ $? -ne 0 ]; then`\n\n    `echo \"An error occurred!\"`\n\n`fi`\n\n3. **Keep It Modular**: Break down complex tasks into smaller functions.  \n4. **Test Thoroughly**: Run scripts in a safe environment before deploying.\n\nHere is more Advanced course on Datacamp \u201cData Processing in Shell\u201d if you fancy to Learn powerful command-line skills to download, process, and transform data, including machine learning pipeline:\n\nhttps://datacamp.pxf.io/qzgzAy"
  },
  {
      "id": 9,
      "section": "## Module 7: Cron Jobs \u2013 Your Automated Alarm Clock\n\n**What it is:** Cron jobs are like setting a timer for your oven, but for scripts and commands.\n\n**What you need to know:**\n\n* Write cron expressions (`* * * * *` format) to schedule tasks.  \n* Automate tasks like data backups or running ETL pipelines.  \n* Debug why your job didn\u2019t run (spoiler: it\u2019s probably a typo).\n\n### **What Is a Cron Job?**\n\nA cron job is a scheduled task that runs at a specific time or interval. These are managed by a utility called **cron**, and the scheduling is defined in a file called the **crontab** (short for \"cron table\").\n\n---\n\n### **Why Cron Jobs Are Essential for Data Engineers**\n\n1. **Scheduled Workflows**: Automate ETL processes to run daily, hourly, or even every minute.  \n2. **Data Integrity**: Schedule regular data validations or checks.  \n3. **Maintenance**: Automate backups, log rotations, or server cleanups.  \n4. **Monitoring**: Set up alerts or monitoring scripts to run at intervals.\n\n---\n\n### **Setting Up Cron Jobs**\n\n#### **1\\. Edit the Crontab**\n\nTo create or edit cron jobs, use:\n\nbash\n\nCopy code\n\n`crontab -e`\n\nThis opens the crontab editor, where you can define your tasks.\n\n---\n\n#### **2\\. Understand Cron Syntax**\n\nCron jobs use a five-field format:\n\n\\`\\`\\`\n\nlua\n\n`* * * * * command_to_run`\n\n`| | | | |`\n\n`| | | | +---- Day of the week (0 - 7, Sunday is both 0 and 7)`\n\n`| | | +------ Month (1 - 12)`\n\n`| | +-------- Day of the month (1 - 31)`\n\n`| +---------- Hour (0 - 23)`\n\n`+------------ Minute (0 - 59)`\n\n\\`\\`\\`\n\nExamples:\n\nRun a script every day at 2 AM:  \nbash  \nCopy code  \n`0 2 * * * /path/to/script.sh`\n\n* \n\nRun every Monday at 6 PM:  \nbash  \nCopy code  \n`0 18 * * 1 /path/to/script.sh`\n\n* \n\nRun every 15 minutes:  \nbash  \nCopy code  \n`*/15 * * * * /path/to/script.sh`\n\n* \n\nRun only in January:  \nbash  \nCopy code  \n`0 10 * 1 * /path/to/script.sh`\n\n* \n\n---\n\n#### **3\\. List Existing Cron Jobs**\n\nTo see all scheduled cron jobs for the current user:\n\nbash\n\nCopy code\n\n`crontab -l`\n\n---\n\n#### **4\\. Remove a Cron Job**\n\nEdit the crontab (`crontab -e`), delete the line with the job, and save.\n\n---\n\n### **Tips and Tricks**\n\n#### **1\\. Logging Cron Jobs**\n\nAdd logging to see what your cron job is doing:\n\nbash\n\nCopy code\n\n`* * * * * /path/to/script.sh >> /path/to/logfile.log 2>&1`\n\nThis appends standard output (logs) and standard error (errors) to `logfile.log`.\n\n---\n\n#### **2\\. Test Before Scheduling**\n\nBefore adding your script to cron, test it manually:\n\nbash\n\nCopy code\n\n`bash /path/to/script.sh`\n\n---\n\n#### **3\\. Environment Variables in Cron**\n\nCron jobs have a limited environment. If your script depends on specific paths or variables, set them explicitly:\n\nbash\n\nCopy code\n\n`PATH=/usr/local/bin:/usr/bin:/bin`\n\n`0 2 * * * /path/to/script.sh`\n\n---\n\n### **Advanced Cron Features**\n\n#### **1\\. Use `@` Keywords**\n\nInstead of fiddling with numbers, use these shortcuts:\n\n* `@reboot`: Run once at system startup.  \n* `@daily`: Same as `0 0 * * *`.  \n* `@hourly`: Same as `0 * * * *`.  \n* `@weekly`: Same as `0 0 * * 0`.  \n* `@monthly`: Same as `0 0 1 * *`.\n\nExample:\n\nbash\n\nCopy code\n\n`@daily /path/to/daily_script.sh`\n\n#### **2\\. Schedule Multiple Users' Cron Jobs**\n\nCron jobs for different users can be managed by system administrators in `/etc/crontab`.\n\n---\n\n### **Common Data Engineering Use Cases**\n\n**ETL Pipelines**  \nRun ETL scripts to process data at regular intervals:  \nbash  \nCopy code  \n`0 3 * * * /home/natalie/scripts/etl_pipeline.sh`\n\n1. \n\n**Database Backups**  \nAutomate daily database dumps:  \nbash  \nCopy code  \n`0 1 * * * mysqldump -u username -p password dbname > /backup/db_$(date +\\%F).sql`\n\n2. \n\n**Log Rotation**  \nArchive and compress logs every week:  \nbash  \nCopy code  \n`0 0 * * 0 tar -czf /logs/archive_$(date +\\%F).tar.gz /logs/current/*`\n\n3. \n\n**Monitoring Scripts**  \nSend alerts if disk usage exceeds a threshold:  \nbash  \nCopy code  \n`*/5 * * * * /home/natalie/scripts/disk_monitor.sh`\n\n4. \n\n---\n\n### **Debugging Cron Jobs**\n\n* **Check Cron Logs**:  \n  Most systems log cron activity in `/var/log/cron` or `/var/log/syslog`.  \n* **Output Not Appearing?**: Redirect output to a file to see what\u2019s happening.\n\n**Permissions**: Ensure the script and its files are executable and accessible.  \nbash  \nCopy code  \n`chmod +x /path/to/script.sh`\n\n* \n\n---\n\n### **Best Practices**\n\n1. **Be Specific**: Clearly define schedules to avoid overloading the system.  \n2. **Test First**: Debug scripts manually before adding them to cron.  \n3. **Document Everything**: Add comments explaining each cron job:  \n   bash  \n   Copy code  \n   `# Run ETL pipeline daily at 3 AM`\n\n`0 3 * * * /home/natalie/scripts/etl_pipeline.sh`\n\n4. **Use Absolute Paths**: Cron doesn\u2019t always know where things are.  \n5. **Monitor Performance**: Ensure jobs don\u2019t overlap or hog system resources.\n\n---\n\n### **Why Cron Jobs Matter for Data Engineers**\n\n* They\u2019re essential for maintaining data workflows, ensuring uptime, and automating repetitive tasks.  \n* With cron jobs, you can set your systems on autopilot and focus on solving bigger problems (or grabbing an extra coffee).\n\n#"
  },
  {
      "id": 10,
      "section": "# Day 7-9: SQL \u2013 The Language of Data\n\nSQL (Structured Query Language) is the bread and butter of data engineering. It\u2019s the language you use to talk to databases, and it\u2019s essential for extracting, transforming, and managing data. Whether you're building pipelines, running analyses, or optimizing storage, SQL is your go-to tool.\n\n---\n\n### **Why Data Engineers Love SQL**\n\n1. **Data Extraction**: Pull data from databases quickly and precisely.  \n2. **Data Transformation**: Clean, join, and aggregate data for downstream processes.  \n3. **Data Storage**: Create and manage schemas, tables, and indexes.  \n4. **Performance Tuning**: Optimize queries and database performance.\n\n---\n\n### **What You Need to Know**\n\n#### **1\\. SQL Basics**\n\n**Querying Data**: Fetch data with the `SELECT` statement.  \nsql  \nCopy code  \n`SELECT column1, column2 FROM table_name WHERE condition;`\n\n* \n\n**Filtering**: Narrow down your results with `WHERE`.  \nsql  \nCopy code  \n`SELECT * FROM users WHERE age > 30;`\n\n* \n\n**Sorting**: Arrange data with `ORDER BY`.  \nsql  \nCopy code  \n`SELECT name, age FROM users ORDER BY age DESC;`\n\n* \n\n**Aggregations**: Summarize data using functions like `SUM`, `COUNT`, `AVG`, etc.  \nsql  \nCopy code  \n`SELECT COUNT(*) AS user_count FROM users WHERE active = 1;`\n\n* \n\n---\n\n#### **2\\. Advanced Querying**\n\n**Joins**: Combine data from multiple tables.  \nsql  \nCopy code  \n`SELECT orders.id, customers.name`  \n`FROM orders`  \n`INNER JOIN customers ON orders.customer_id = customers.id;`\n\n*   \n  * Types of joins:  \n    * `INNER JOIN`: Matches rows in both tables.  \n    * `LEFT JOIN`: All rows from the left table, with matching rows from the right.  \n    * `RIGHT JOIN`: All rows from the right table, with matching rows from the left.  \n    * `FULL OUTER JOIN`: All rows from both tables.\n\n**Subqueries**: Use a query inside another query.  \nsql  \nCopy code  \n`SELECT name FROM users WHERE id IN (SELECT user_id FROM orders);`\n\n* \n\n**Common Table Expressions (CTEs)**: Simplify complex queries.  \nsql  \nCopy code  \n`WITH top_customers AS (`  \n    `SELECT customer_id, SUM(amount) AS total_spent`  \n    `FROM orders`  \n    `GROUP BY customer_id`  \n    `HAVING total_spent > 1000`  \n`)`  \n`SELECT * FROM top_customers;`\n\n* \n\n**Window Functions**: Perform calculations across a set of table rows.  \nsql  \nCopy code  \n`SELECT name, salary, RANK() OVER (ORDER BY salary DESC) AS rank`  \n`FROM employees;`\n\n* \n\n---\n\n#### **3\\. Data Definition Language (DDL)**\n\n**Create Tables**:  \nsql  \nCopy code  \n`CREATE TABLE employees (`  \n    `id INT PRIMARY KEY,`  \n    `name VARCHAR(100),`  \n    `salary DECIMAL(10, 2),`  \n    `department_id INT`  \n`);`\n\n* \n\n**Alter Tables**:  \nsql  \nCopy code  \n`ALTER TABLE employees ADD COLUMN hire_date DATE;`\n\n* \n\n**Drop Tables**:  \nsql  \nCopy code  \n`DROP TABLE employees;`\n\n* \n\n---\n\n#### **4\\. Data Manipulation Language (DML)**\n\n**Insert Data**:  \nsql  \nCopy code  \n`INSERT INTO employees (id, name, salary, department_id)`  \n`VALUES (1, 'Natalie', 70000, 2);`\n\n* \n\n**Update Data**:  \nsql  \nCopy code  \n`UPDATE employees SET salary = 75000 WHERE id = 1;`\n\n* \n\n**Delete Data**:  \nsql  \nCopy code  \n`DELETE FROM employees WHERE id = 1;`\n\n* \n\n---\n\n#### **5\\. Performance Optimization**\n\n**Indexes**: Speed up queries by indexing frequently searched columns.  \nsql  \nCopy code  \n`CREATE INDEX idx_name ON employees(name);`\n\n* \n\n**Query Analysis**: Use `EXPLAIN` or `EXPLAIN ANALYZE` to understand query performance.  \nsql  \nCopy code  \n`EXPLAIN SELECT * FROM employees WHERE name = 'Natalie';`\n\n* \n\n**Partitioning**: Split large tables into smaller, manageable chunks.  \nsql  \nCopy code  \n`CREATE TABLE employees_partitioned (`  \n    `id INT,`  \n    `name VARCHAR(100),`  \n    `salary DECIMAL(10, 2),`  \n    `department_id INT`  \n`) PARTITION BY RANGE (department_id);`\n\n*   \n* **Normalization**: Organize data to reduce redundancy and improve integrity.  \n* **Denormalization**: Optimize for read-heavy workloads by flattening data.\n\n---\n\n#### **6\\. Database Management**\n\n**Backup and Restore**: Always have a backup plan.  \nbash  \nCopy code  \n`# Example with PostgreSQL`  \n`pg_dump dbname > backup.sql`  \n`psql dbname < backup.sql`\n\n* \n\n**User Management**: Control access with permissions.  \nsql  \nCopy code  \n`GRANT SELECT ON employees TO read_only_user;`  \n`REVOKE DELETE ON employees FROM read_only_user;`\n\n*   \n* **Monitoring**: Keep an eye on performance and resource usage.\n\n---\n\n### **Common SQL Tools for Data Engineers**\n\n1. **PostgreSQL**: Open-source relational database with powerful features.  \n2. **MySQL/MariaDB**: Lightweight and widely used.  \n3. **SQLite**: Great for prototyping or lightweight applications.  \n4. **BigQuery**: Google's serverless data warehouse.  \n5. **Snowflake**: Cloud-based, scalable data warehouse.\n\n# \n\nHere is Free Introduction to SQL course on Datacamp:  \n[https://www.datacamp.com/courses/introduction-to-sql](https://www.datacamp.com/courses/introduction-to-sql)\n\nOr if you want fun SQL game \u201cSQL police deparment\u201d: [https://sqlpd.com/](https://sqlpd.com/)  \nOr even common sql interview questions for data engineers: [https://www.interviewquery.com/questions?searchQuery=\\&searchQuestionTag=\\&searchCompany=\\&ordering=Recommended\\&orderingDirection=ASC\\&pageSize=20\\&page=0\\&tags=SQL\\&positions=Data+Engineer\\&positions=Data+Analyst\\&levels=1\\&levels=2\\&levels=3](https://www.interviewquery.com/questions?searchQuery=&searchQuestionTag=&searchCompany=&ordering=Recommended&orderingDirection=ASC&pageSize=20&page=0&tags=SQL&positions=Data+Engineer&positions=Data+Analyst&levels=1&levels=2&levels=3)"
  },
  {
      "id": 11,
      "section": "# Day 10: **SQL vs NoSQL**\n\n### **SQL vs NoSQL \u2013 The Ultimate Database Showdown**\n\nWhen it comes to databases, you\u2019ve got two main contenders in the ring: **SQL (Structured Query Language)** and **NoSQL (Not Only SQL)**. Think of them like two superhero teams with different powers\u2014each great for specific scenarios.\n\n---\n\n### **SQL Databases \u2013 The Organized Type**\n\nSQL databases are relational. They store data in structured tables with rows and columns, much like an Excel spreadsheet. Relationships between data are defined through keys (primary and foreign).\n\n#### **Strengths of SQL**\n\n1. **Structure**: Perfect for when data fits neatly into rows and columns.  \n2. **ACID Compliance**: Ensures transactions are consistent, even in failure (Atomicity, Consistency, Isolation, Durability).  \n3. **Mature Ecosystem**: Tools like PostgreSQL, MySQL, and Oracle DB are robust and well-documented.  \n4. **Complex Queries**: Supports intricate operations like joins, aggregations, and nested queries.\n\n#### **Weaknesses of SQL**\n\n* **Rigid Schema**: Changes to the structure (e.g., adding columns) can be cumbersome.  \n* **Scalability**: Vertical scaling (buying a bigger server) can get expensive.\n\n---\n\n### **NoSQL Databases \u2013 The Free Spirits**\n\nNoSQL databases are non-relational. They store data in a flexible format like key-value pairs, documents, graphs, or wide-column stores.\n\n#### **Strengths of NoSQL**\n\n1. **Flexibility**: Great for unstructured or semi-structured data.  \n2. **Horizontal Scaling**: Distribute data across multiple servers easily.  \n3. **Variety**: Different types for different needs (e.g., MongoDB for documents, Neo4j for graphs).  \n4. **Speed**: Often faster for large-scale read/write operations.\n\n#### **Weaknesses of NoSQL**\n\n* **Lack of Standardization**: Every NoSQL database has its own query language.  \n* **Limited Complex Queries**: Not as powerful for relational operations.  \n* **Weaker Consistency**: Some NoSQL databases favor speed and availability over strict data consistency.\n\n---\n\n### **Head-to-Head Comparison**\n\n| Aspect | SQL | NoSQL |\n| ----- | ----- | ----- |\n| **Schema** | Fixed schema, predefined tables | Dynamic schema, flexible structure |\n| **Scalability** | Vertical scaling | Horizontal scaling |\n| **Data Relationships** | Strong, using joins and keys | Weak or absent, modeled differently |\n| **Use Case** | Structured, consistent data | Unstructured, rapidly changing data |\n| **Examples** | MySQL, PostgreSQL, Oracle, SQL Server | MongoDB, Cassandra, Redis, Neo4j |\n| **Query Language** | SQL (standardized) | Varies (e.g., MongoDB uses BSON) |\n| **Consistency** | Strong (ACID) | Eventual consistency (CAP theorem) |\n| **Performance** | May struggle with massive data volumes | Optimized for large-scale operations |\n\n---\n\n### **When to Choose SQL**\n\n1. **Structured Data**: If your data fits into a neat table structure.  \n2. **Transactions**: For banking, e-commerce, or any use case requiring strict consistency.  \n3. **Mature Systems**: When you need proven reliability and enterprise-grade support.\n\n---\n\n### **When to Choose NoSQL**\n\n1. **Unstructured Data**: For data like JSON documents, graphs, or logs.  \n2. **Scalability**: When you need to handle massive, distributed datasets.  \n3. **Rapid Development**: If your schema evolves frequently or is undefined at the start.  \n4. **Specialized Use Cases**:  \n   * **MongoDB**: Flexible document stores (e.g., content management).  \n   * **Cassandra**: High write throughput (e.g., IoT data).  \n   * **Neo4j**: Complex relationships (e.g., social networks).\n\n---\n\n### **Can SQL and NoSQL Work Together?**\n\nAbsolutely\\! Many systems use both:\n\n* SQL for transactional data (e.g., user accounts).  \n* NoSQL for big data or real-time analytics (e.g., tracking clicks on a website).\n\n---\n\n### **SQL vs NoSQL Cheat Sheet**\n\n* **E-commerce**: SQL (for orders, inventory) \\+ NoSQL (for session data).  \n* **Social Media**: SQL (for user profiles) \\+ NoSQL (for feed updates).  \n* **Big Data**: Mostly NoSQL (e.g., Hadoop, Cassandra).\n\n---\n\n### **Pro Tips for Data Engineers**\n\n1. **Understand the Use Case**: Choose SQL for structured, reliable operations; NoSQL for flexibility and scalability.  \n2. **Learn Both**: Many companies use hybrid systems, so it\u2019s handy to know SQL and at least one NoSQL database.  \n3. **Optimize for Performance**: Use SQL indexes and NoSQL partitioning wisely.  \n4. **Stay Curious**: Explore new tools\u2014SQL and NoSQL ecosystems are evolving rapidly.\n\nHere is a cool article about it: https://pub.towardsai.net/sql-vs-nosql-choose-the-most-convenient-technology-4506d831b6e4\n\n# \n\n#"
  },
  {
      "id": 12,
      "section": "# Day 11-13: Foundations of Data Modeling\n\n## Module 1: What's the Big Deal About Data Modeling?\n\nEver tried to build a house without a blueprint? That's what working with data without a proper model is like. Let's dive into why data modeling is the superhero of the data world\\!\n\n1. Definition: Data modeling is like creating a map of your data universe. It's the process of visualizing how data elements relate to each other, just like how you'd plan out the rooms in your dream house.  \n2. Importance:  \n   * Improves data quality (no more \"garbage in, garbage out\"\\!)  \n   * Boosts system performance (your queries will zoom\\!)  \n   * Helps stakeholders speak the same language (no more \"data lost in translation\")  \n3. Types of Data Models:  \n   * Conceptual: The \"big picture\" view  \n   * Logical: The \"how it all fits together\" view  \n   * Physical: The \"nuts and bolts\" view\n\nTip: Always start with a conceptual model. It's like sketching your idea on a napkin before building the Eiffel Tower\\!  \nNow, tell me what you've understood about data modeling so far. Why do you think it's important, and can you name the three types of data models?"
  },
  {
      "id": 13,
      "section": "## Module 2: Entity-Relationship Diagrams (ERD) \\- Drawing Data Relationships\n\nNow that we know why data modeling is crucial, let's learn how to draw these data masterpieces. Enter the Entity-Relationship Diagram, or ERD for short.\n\n1. Entities: These are the nouns of your data world. Think \"Customer\", \"Product\", \"Order\". In code, they often become tables.\n\nsql\n\n`CREATE TABLE Customer (`  \n  `CustomerID INT PRIMARY KEY,`  \n  `Name VARCHAR(100),`  \n  `Email VARCHAR(100)`  \n`);`\n\n2. Attributes: The characteristics of your entities. For a Customer, it might be Name, Email, etc.  \n3. Relationships: How entities interact. A Customer places an Order, an Order contains Products.\n\ntext\n\n`Customer --< Order >-- Product`\n\nThis says \"A Customer can have many Orders, and an Order can have many Products\"\n\n4. Cardinality: The numerical nature of relationships. One-to-One, One-to-Many, Many-to-Many.\n\nTip: When drawing ERDs, always use a pencil first. Erasing is easier than starting over, trust me\\! \ud83d\ude05  \nJoke time: Why did the database administrator leave the party? He wanted to establish a better relationship\\! \ud83e\udd41\n\nAlright, data padawan, your turn. Can you explain what entities and attributes are in your own words? And how about giving an example of a relationship you might find in a real-world scenario?"
  },
  {
      "id": 14,
      "section": "## Module 3: Normalization and Denormalization\n\n1. What is Normalization?: It's the process of organizing data to reduce redundancy and improve data integrity. Think of it as decluttering your data closet.  \n2. First Normal Form (1NF):  \n   * Each column contains atomic values  \n   * No repeating groups\n\n   Before 1NF:\n\ntext\n\n`| OrderID | Products         |`  \n`|---------|------------------|`  \n`| 1       | Apple, Banana    |`\n\nAfter 1NF:\n\ntext\n\n`| OrderID | Product |`  \n`|---------|---------|`  \n`| 1       | Apple   |`  \n`| 1       | Banana  |`\n\n3. Second Normal Form (2NF):  \n   * Must be in 1NF  \n   * All non-key attributes depend on the entire primary key\n\n   \n\n4. Third Normal Form (3NF):  \n   * Must be in 2NF  \n   * No transitive dependencies\n\n   \n\nTip: Remember, normalization is about reducing redundancy, not about performance. It's like eating your vegetables \\- good for you in the long run\\!  \nJoke: Why did the database go to the psychiatrist? It had too many relationships and wasn't feeling normalized\\! \ud83d\ude02\n\n1. What is Denormalization?: It's the process of adding redundant data to one or more tables to improve read performance. It's like creating a messy \"everything drawer\" in your kitchen for quick access.  \n2. When to Denormalize:  \n   * When you need blazing fast read performance  \n   * When your data is mostly static  \n   * When you're dealing with reporting and analytics  \n3. How to Denormalize:  \n   * Combine tables  \n   * Add redundant columns  \n   * Pre-calculate values\n\nExample:  \nNormalized:  \nsql  \n`SELECT o.OrderID, c.CustomerName, p.ProductName`  \n`FROM Orders o`  \n`JOIN Customers c ON o.CustomerID = c.CustomerID`  \n`JOIN OrderDetails od ON o.OrderID = od.OrderID`  \n`JOIN Products p ON od.ProductID = p.ProductID;`\n\nDenormalized:  \nsql  \n`SELECT OrderID, CustomerName, ProductName`  \n`FROM DenormalizedOrders;`\n\nTip: Always measure performance before and after denormalization. Sometimes, what looks like a shortcut is actually a scenic route\\!  \nJoke: Why did the denormalized database blush? Because everyone could see its redundancies\\! \ud83e\udd2d  \nAlright, data rule-breaker, your turn\\! Can you explain when we might want to denormalize our data? And can you think of a real-world scenario where denormalization might be useful?"
  },
  {
      "id": 15,
      "section": "## Module 4: Dimensional Modeling: Star Schema \n\nNow, we're diving into dimensional modeling, starting with the glamorous Star Schema.\n\n1. What is a Star Schema?: It's a design pattern with a central fact table connected to multiple dimension tables. Think of it as a star-shaped diagram, hence the name.  \n2. Fact Table:  \n   * Contains measurable, quantitative data  \n   * Usually numeric and additive  \n3. Example:\n\nsql\n\n`CREATE TABLE FactSales (`  \n  `SaleID INT PRIMARY KEY,`  \n  `DateID INT,`  \n  `ProductID INT,`  \n  `StoreID INT,`  \n  `CustomerID INT,`  \n  `SalesAmount DECIMAL(10,2),`  \n  `Quantity INT`  \n`);`\n\n4. Dimension Tables:  \n   * Contain descriptive attributes  \n   * Provide context to the facts\n\nExample:\n\nsql\n\n`CREATE TABLE DimProduct (`  \n  `ProductID INT PRIMARY KEY,`  \n  `ProductName VARCHAR(100),`  \n  `Category VARCHAR(50),`  \n  `Brand VARCHAR(50)`  \n`);`\n\nTip: When designing a star schema, start with the fact table. It's like picking the lead actor for your data movie\\!  \nJoke: Why did the fact table feel lonely? Because all its friends were in another dimension\\! \ud83d\ude04  \nNow, data director, it's your turn\\! Can you explain what a star schema is and the difference between fact and dimension tables? Can you think of a real-world scenario where a star schema might be useful?"
  },
  {
      "id": 16,
      "section": "## Module 5: Snowflake Schema \\- Let It Snow, Let It Snow, Let It Snow\\!\n\nNow that we've mastered the star schema, let's make it snow with the snowflake schema.\n\n1. What is a Snowflake Schema?: It's an extension of the star schema where dimension tables are normalized into multiple related tables. It's like if each point of your star grew more points\\!  \n2. Differences from Star Schema:  \n   * Dimension tables are normalized  \n   * More complex joins  \n   * Can save storage space  \n3. Example:\n\nInstead of one DimProduct table, we might have:\n\nsql\n\n`CREATE TABLE DimProduct (`  \n  `ProductID INT PRIMARY KEY,`  \n  `ProductName VARCHAR(100),`  \n  `CategoryID INT`  \n`);`\n\n`CREATE TABLE DimCategory (`  \n  `CategoryID INT PRIMARY KEY,`  \n  `CategoryName VARCHAR(50),`  \n  `DepartmentID INT`  \n`);`\n\n`CREATE TABLE DimDepartment (`  \n  `DepartmentID INT PRIMARY KEY,`  \n  `DepartmentName VARCHAR(50)`  \n`);`\n\nTip: Snowflake schemas can save space, but at the cost of query complexity. It's like folding your clothes really small \\- saves space, but takes longer to get dressed\\!\n\nJoke: Why did the snowflake schema catch a cold? It had too many dimensions\\! \ud83e\udd27\n\nAlright, data snowflake, your turn to shine\\! Can you explain how a snowflake schema differs from a star schema? Can you think of a situation where you might prefer a snowflake schema over a star schema?"
  },
  {
      "id": 17,
      "section": "## Module 6: Slowly Changing Dimensions (SCDs) \\- When Change is the Only Constant\n\nGood morning, data time traveler\\! Today we're diving into the fascinating world of Slowly Changing Dimensions (SCDs). Buckle up, we're about to bend time\\!\n\n1. What are SCDs?: They're dimension tables that handle changes to dimension data over time. It's like keeping a diary for your data\\!  \n2. Types of SCDs:  \n   * Type 1: Overwrite (No history kept)  \n   * Type 2: Add new row (Full history)  \n   * Type 3: Add new attribute (Limited history)  \n3. SCD Type 2 Example:\n\nsql\n\n`CREATE TABLE DimCustomer (`  \n  `CustomerKey INT PRIMARY KEY,`  \n  `CustomerID INT,`  \n  `Name VARCHAR(100),`  \n  `Email VARCHAR(100),`  \n  `ValidFrom DATE,`  \n  `ValidTo DATE,`  \n  `IsCurrent BIT`  \n`);`\n\n4. When a customer's email changes:\n\nsql\n\n*`-- Set the current record as inactive`*  \n`UPDATE DimCustomer`  \n`SET ValidTo = GETDATE(), IsCurrent = 0`  \n`WHERE CustomerID = 123 AND IsCurrent = 1;`\n\n*`-- Insert the new record`*  \n`INSERT INTO DimCustomer (CustomerID, Name, Email, ValidFrom, ValidTo, IsCurrent)`  \n`VALUES (123, 'John Doe', 'newemail@example.com', GETDATE(), NULL, 1);`\n\nTip: When implementing SCDs, always consider the business requirements. Sometimes, history is important; other times, it's just baggage\\!\n\nJoke: Why did the SCD Type 2 dimension feel like a superhero? Because it could be in two places at once\\! \ud83e\uddb8\u200d\u2642\ufe0f  \nNow, time lord of data, it's your turn\\! Can you explain what Slowly Changing Dimensions are and why they're important? Can you describe the difference between SCD Type 1 and Type 2?"
  },
  {
      "id": 18,
      "section": "## Module 7: Data Vault Modeling \\- The Swiss Army Knife of Data Modeling\n\nWelcome back, data vault hunter\\! Now we're going to explore Data Vault modeling, a flexible approach to handle complex and changing environments.\n\n1. What is Data Vault?: It's a hybrid approach that combines the best of 3NF and dimensional modeling. Think of it as the Swiss Army knife of data modeling\\!  \n2. Key Components:  \n   * Hubs: Business keys (unique identifiers)  \n   * Links: Relationships between hubs  \n   * Satellites: Descriptive attributes  \n3. Example:\n\nsql\n\n*`-- Hub`*  \n`CREATE TABLE Hub_Customer (`  \n  `Hub_Customer_Key INT PRIMARY KEY,`  \n  `Customer_ID VARCHAR(50),`  \n  `Load_Date DATETIME,`  \n  `Record_Source VARCHAR(100)`  \n`);`\n\n*`-- Link`*  \n`CREATE TABLE Link_Customer_Order (`  \n  `Link_Customer_Order_Key INT PRIMARY KEY,`  \n  `Hub_Customer_Key INT,`  \n  `Hub_Order_Key INT,`  \n  `Load_Date DATETIME,`  \n  `Record_Source VARCHAR(100)`  \n`);`\n\n*`-- Satellite`*  \n`CREATE TABLE Sat_Customer (`  \n  `Sat_Customer_Key INT PRIMARY KEY,`  \n  `Hub_Customer_Key INT,`  \n  `Name VARCHAR(100),`  \n  `Email VARCHAR(100),`  \n  `Load_Date DATETIME,`  \n  `Record_Source VARCHAR(100)`  \n`);`\n\nTip: Data Vault is great for its flexibility, but it comes with complexity. It's like having a Swiss Army knife \\- versatile, but you need to know which tool to use when\\!  \nJoke: Why did the Data Vault model feel so secure? Because it kept all its valuables in different vaults\\! \ud83d\udd12  \nAlright, data vault cracker, your turn\\! Can you explain the main components of a Data Vault model? How do you think Data Vault differs from the star or snowflake schemas we learned about earlier?"
  },
  {
      "id": 19,
      "section": "## Module 8: One Big Table (OBT) \\- When Bigger is Better\n\nWelcome back, data maximalist\\! Now we're going to explore the One Big Table (OBT) approach, which is like the all-you-can-eat buffet of data modeling\\!\n\n1\\. What is One Big Table?: OBT is a data modeling approach that involves denormalizing and combining all your data into one wide, flat table. It's like taking all your Lego pieces and gluing them into one giant, colorful slab\\!\n\n2\\. Key Concepts:  \n   \\- Denormalization: Combining data from multiple tables into one  \n   \\- Wide Tables: Tables with many columns (sometimes hundreds or thousands)  \n   \\- Complex Data Types: Using arrays and structs to nest data within columns\n\n3\\. OBT Example:  \n   Let's model a simple e-commerce scenario:\n\n   \\`\\`\\`sql  \n   CREATE TABLE one\\_big\\_table (  \n     order\\_id INT,  \n     order\\_date TIMESTAMP,  \n     customer\\_id INT,  \n     customer\\_name STRING,  \n     customer\\_email STRING,  \n     product\\_id INT,  \n     product\\_name STRING,  \n     product\\_category STRING,  \n     quantity INT,  \n     price DECIMAL(10,2),  \n     order\\_status ARRAY\\<STRUCT\\<status: STRING, timestamp: TIMESTAMP\\>\\>  \n   );  \n   \\`\\`\\`\n\n   In this model:  \n   \\- We've combined order, customer, and product information into one table  \n   \\- We're using an ARRAY of STRUCTs to store order status history\n\n4\\. Querying OBT (using SQL):\n\n   \\`\\`\\`sql  \n   SELECT   \n     customer\\_name,  \n     SUM(quantity \\* price) as total\\_spent,  \n     ARRAY\\_AGG(DISTINCT product\\_category) as categories\\_purchased  \n   FROM one\\_big\\_table  \n   WHERE order\\_date \\>= DATE\\_SUB(CURRENT\\_DATE(), INTERVAL 30 DAY)  \n   GROUP BY customer\\_id, customer\\_name;  \n   \\`\\`\\`\n\nTip: OBT shines when you're dealing with massive amounts of data and need to optimize for read performance. It's like building a race car \\- streamlined for speed, but maybe not the most flexible for a family road trip\\!\n\nJoke: Why did the One Big Table feel like it was at a buffet? Because it had a little bit of everything\\! \ud83c\udf7d\ufe0f\n\nAlright, OBT optimizer, it's your turn\\! Can you explain some advantages and disadvantages of the One Big Table approach? How do you think OBT might compare to the star schema we learned about earlier in terms of query performance and data management?"
  },
  {
      "id": 20,
      "section": "## Module 9: NoSQL Data Modeling \\- Breaking Free from Tables\n\nGood morning, data rebel\\! Today we're breaking free from the constraints of relational databases and diving into the wild world of NoSQL data modeling.\n\n1. What is NoSQL?: It stands for \"Not Only SQL\" and refers to non-relational databases. It's like the free-form jazz of the database world\\!  \n2. Types of NoSQL Databases:  \n   * Key-Value Stores (e.g., Redis)  \n   * Document Databases (e.g., MongoDB)  \n   * Column-Family Stores (e.g., Cassandra)  \n   * Graph Databases (e.g., Neo4j)  \n3. Document Database Example (MongoDB):\n\njavascript\n\n*`// Customer document`*  \n`{`  \n  `\"_id\": ObjectId(\"5099803df3f4948bd2f98391\"),`  \n  `\"name\": \"Joe Schmoe\",`  \n  `\"email\": \"joe@schmoe.com\",`  \n  `\"orders\": [`  \n    `{`  \n      `\"date\": ISODate(\"2020-01-01T00:00:00Z\"),`  \n      `\"items\": [`  \n        `{ \"product\": \"Widget\", \"quantity\": 1, \"price\": 9.99 },`  \n        `{ \"product\": \"Gadget\", \"quantity\": 2, \"price\": 12.50 }`  \n      `]`  \n    `}`  \n  `]`  \n`}`\n\nTip: When modeling for NoSQL, think about your access patterns. It's like organizing your closet based on how often you wear things\\!  \nJoke: Why did the NoSQL database feel so relaxed? Because it had no relations to worry about\\! \ud83d\ude0e  \nNow, NoSQL ninja, it's your turn\\! Can you explain what NoSQL databases are and how they differ from relational databases? Can you think of a scenario where a document database might be more suitable than a traditional relational database?"
  },
  {
      "id": 21,
      "section": "## Module 10: Data Modeling for Data Lakes \\- Diving into the Data Deep End\n\nWelcome back, data lake lifeguard\\! Now we're going to explore how to model data for the vast and sometimes murky waters of data lakes.\n\n1. What is a Data Lake?: It's a storage repository that holds a vast amount of raw data in its native format. Think of it as a giant pool where you can dump all your data\\!  \n2. Data Lake Zones:  \n   * Raw Zone (Bronze): Data in original format  \n   * Processed Zone (Silver): Cleansed and transformed data  \n   * Curated Zone (Gold): Business-ready data  \n3. Delta Lake Concept:  \n   Delta Lake is an open-source storage layer that brings ACID transactions to big data workloads.  \n4. Example (using Spark):  \n     \n   python\n\n*`# Write data to Delta Lake`*  \n`data.write.format(\"delta\").save(\"/path/to/delta-table\")`\n\n*`# Read from Delta Lake`*  \n`df = spark.read.format(\"delta\").load(\"/path/to/delta-table\")`\n\n*`# Time travel`*  \n`df = spark.read.format(\"delta\").option(\"versionAsOf\", 5).load(\"/path/to/delta-table\")`\n\nTip: When modeling for data lakes, think about how you'll need to process and analyze the data later. It's like packing for a trip \\- you need to balance between bringing everything and being able to find what you need\\!"
  }
];